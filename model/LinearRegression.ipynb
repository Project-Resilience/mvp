{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb59a10b",
   "metadata": {},
   "source": [
    "# Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc193b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xbatcher\n",
    "import regionmask\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8b171",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aecead8",
   "metadata": {},
   "source": [
    "Use this configuration file to define the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee27a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"data_source_url\": \"zip:///::https://huggingface.co/datasets/jacobbieker/project-resilience/resolve/main/merged_aggregated_dataset.zarr.zip\",\n",
    "    \"data_source_path\": \"../data/gcb/raw/zarr/merged_aggregated_dataset.zarr\",\n",
    "    \"start_year\": 2000,\n",
    "    \"end_year\": 2020,\n",
    "    \"test_years\": [2021, 2022],\n",
    "    \"target\": \"ELUC_diff\",\n",
    "    \"batch_size\": 1,\n",
    "    \"n_timepoint_in_each_sample\": 10,\n",
    "    \"n_timepoint_in_each_batch\": 5,\n",
    "    \"n_latlon_in_each_sample\": 128,\n",
    "    \"n_latlon_in_each_batch\": 1,\n",
    "    \"input_overlap\": 1,\n",
    "#    \"countries\": regionmask.defined_regions.natural_earth_v5_0_0.countries_110.names # select all\n",
    "#    \"countries\": [\"United States of America\", \"France\", \"Canada\", \"United Kingdom\"]  # list of countries\n",
    "    \"countries\": [\"United States of America\"] \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b998b",
   "metadata": {},
   "source": [
    "If ELUC dataset is already in your disk, just update the config file (if necessary).\n",
    "If not, here are some steps that will save you some time: \n",
    "\n",
    "- [Download the dataset as a Zip file](https://huggingface.co/datasets/jacobbieker/project-resilience/resolve/main/merged_aggregated_dataset.zarr.zip)\n",
    "- Extract the zip file to following folder: `MVP` > `data` > `gcb` > `raw` > `zarr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELUC_Dataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, conf):\n",
    "        super(ELUC_Dataset).__init__()\n",
    "        self.path = conf[\"data_source_path\"]\n",
    "        self.countries = conf[\"countries\"]\n",
    "        self.target = conf[\"target\"]\n",
    "        # check if eluc dataset is already on disk\n",
    "        if os.path.exists(conf[\"data_source_path\"]):\n",
    "            self.ds = xr.open_dataset(conf[\"data_source_path\"], engine='zarr', chunks={})\n",
    "        else: \n",
    "            self.ds = xr.open_dataset(conf[\"data_source_url\"], engine='zarr', chunks={})\n",
    "            self.ds.to_zarr(conf[\"data_source_path\"], consolidated=True, compute=True)\n",
    "        \n",
    "        self.var_names = list(self.ds.data_vars.keys())\n",
    "        \n",
    "        self.ds = self.ds.stack(latlon=('lat', 'lon'))\n",
    "        country_mask = regionmask.defined_regions.natural_earth_v5_0_0.countries_110.mask(self.ds)\n",
    "        self.ds = self.ds.assign_coords({\"country\":country_mask})\n",
    "        df_countries = regionmask.defined_regions.natural_earth_v5_0_0.countries_110.to_dataframe()\n",
    "        country_numbers = list(df_countries.loc[df_countries.names.isin(conf[\"countries\"])].index.values)\n",
    "        self.ds = self.ds.where((self.ds.time >= conf[\"start_year\"]) & \n",
    "                                (self.ds.time <= conf[\"end_year\"]) & \n",
    "                                (self.ds.country.isin(country_numbers)), \n",
    "                                drop=True)\n",
    "        self.ds = xr.concat([self.ds[var] for var in self.ds.data_vars], dim='variable')\n",
    "        self.ds = self.ds.assign_coords(variable=self.var_names)\n",
    "        self.ds = self.ds.transpose(\"time\", \"latlon\", \"variable\")\n",
    "        self.bgen = xbatcher.BatchGenerator(\n",
    "            ds=self.ds,\n",
    "            input_dims={\"time\": conf[\"n_timepoint_in_each_sample\"]+1, \n",
    "                        \"latlon\": conf[\"n_latlon_in_each_sample\"],\n",
    "                        \"variable\": len(self.var_names)},\n",
    "#            batch_dims={\"time\": conf[\"n_timepoint_in_each_batch\"], \n",
    "#                        \"latlon\": conf[\"n_latlon_in_each_batch\"]},\n",
    "#            batch_dims={\"time\": conf[\"n_timepoint_in_each_batch\"]},\n",
    "#            concat_input_dims=False,\n",
    "            input_overlap={\"time\": conf[\"input_overlap\"]})\n",
    "        display(self.ds)\n",
    "        \n",
    "    def __iter__(self):\n",
    "#        firstb = 0    \n",
    "        for batch in self.bgen:\n",
    "            np_batch = np.squeeze(batch.fillna(0.0).values, axis=None)\n",
    "            np_batch = np.nan_to_num(np_batch, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            pt_batch = torch.from_numpy(np_batch)\n",
    "            if int(torch.count_nonzero(pt_batch))==0:  # ignore batches w zeros only\n",
    "                continue\n",
    "#            if firstb == 0:  # only for debugging\n",
    "#                display(batch)\n",
    "#            firstb = 1\n",
    "            pt_features = pt_batch[:-1,:,:].permute((1, 0, 2)) # all years except the last (0 : year-1)\n",
    "            pt_target = pt_batch[-1:,:,self.var_names.index(self.target)].permute((1, 0)) # select target variable (year)          \n",
    "            yield pt_features, pt_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb5585",
   "metadata": {},
   "source": [
    "Use ELUC_Dataset to iterate over data samples, it will provide a stream of data reading from file.\n",
    "\n",
    "More information available:\n",
    "\n",
    "- [Pytorch IterableDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)\n",
    "- [xbatcher: Batch Generation from Xarray Datasets](https://xbatcher.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b3710",
   "metadata": {},
   "source": [
    "Load `merged_aggregated_dataset.zarr` and iterate ELUC dataset in batches.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722abe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELUC dataset\n",
    "training_data = ELUC_Dataset(train_config)\n",
    "train_dataloader = DataLoader(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c95b5f",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a1a56",
   "metadata": {},
   "source": [
    "Trained a linear regression model to forecast the values of ELUC change in the next year.\n",
    "The training period is between the year 2000 until the year 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c79c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(280,1) \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = torch.nn.MSELoss()\n",
    "model_lin = LinearRegressionModel()\n",
    "optim_lin = torch.optim.Adam(model_lin.parameters(), lr=0.001)\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch: {epoch+1}\")\n",
    "    for i, (ts, target) in enumerate(train_dataloader, 1):\n",
    "        ts = ts.reshape(128, 28 * 10)\n",
    "        target = target.reshape(128,1)\n",
    "        optim_lin.zero_grad() # zero the parameter gradients\n",
    "        outputs_lin = model_lin(ts)    # forward \n",
    "        loss_lin = crit(outputs_lin, target)\n",
    "        loss_lin.backward()                     # backward\n",
    "        optim_lin.step()                        # optimize\n",
    "        if i == 1: \n",
    "            print(f\"batch {i} | loss: {loss_lin.item()} \")\n",
    "        elif i % 10 == 0:\n",
    "            print(f\"batch {i} | loss: {loss_lin.item()} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818769b3",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691686fa",
   "metadata": {},
   "source": [
    "Check `ELUC_diff` forecasts for the years 2021 and 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea8e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = train_config.copy()\n",
    "\n",
    "for year in test_config[\"test_years\"]:\n",
    "    test_config[\"end_year\"] = year\n",
    "    test_config[\"start_year\"] = test_config[\"end_year\"] - test_config[\"n_timepoint_in_each_sample\"]\n",
    "    test_data = ELUC_Dataset(test_config)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=test_config[\"batch_size\"])\n",
    "    test_loss = []\n",
    "    for i, (ts, target) in enumerate(test_dataloader, 1):\n",
    "        ts = ts.reshape(128, 28 * 10)\n",
    "        target = target.reshape(128,1)\n",
    "        outputs_test = model_lin(ts)    # forward \n",
    "        loss = crit(outputs_test, target)\n",
    "        test_loss.append(loss.item())\n",
    "    print(f\"Average loss for {year}: {np.mean(test_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d7e63",
   "metadata": {},
   "source": [
    "## Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb569e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features names: from year_9 until year\n",
    "shifted_vars = [\"_\" + str(s) for s in range(10)][::-1]\n",
    "shifted_vars[-1] = \"\"\n",
    "\n",
    "variable_names = []\n",
    "for shift in shifted_vars:\n",
    "    variable_names.extend([var + shift for var in test_data.var_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf0a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features importance dataframe\n",
    "features_importance = pd.DataFrame({\"Features\":variable_names, \n",
    "                                    \"Importance\": model_lin.linear.weight.data.tolist()[0]},\n",
    "                                   index=variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot centile 0.01 and 0.99\n",
    "c95 = features_importance.Importance.quantile(q=0.95)\n",
    "c05 = features_importance.Importance.quantile(q=0.05)\n",
    "features_importance.loc[(features_importance.Importance<=c05) | \n",
    "                        (features_importance.Importance>=c95)\n",
    "                       ].sort_values(\"Importance\").plot(kind='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
